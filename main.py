__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

#import
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.chains.llm import LLMChain
import tempfile
import os
from streamlit_extras.buy_me_a_coffee import button
import streamlit as st
from langchain.chains.summarize import load_summarize_chain
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import AgentAction, AgentFinish, LLMResult
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.retrievers.multi_query import MultiQueryRetriever

#Stream ë°›ì•„ ì¤„ Hander ë§Œë“¤ê¸°
class StreamHandler(BaseCallbackHandler):
        def __init__(self, container, initial_text=""):
            self.container = container
            self.text=initial_text
        def on_llm_new_token(self, token: str, **kwargs) -> None:
            self.text+=token
            self.container.markdown(self.text)


# from langchain.llms import CTransformers
chat_model = ChatOpenAI(model="gpt-4", temperature=0)
# llm = CTransformers(
#     model="llama-2-7b-chat.ggmlv3.q2_K.bin",
#     model_type="llama"
# )
button(username="sfix", floating=True, width=221)


st.title('AI ë©´ì ‘ ì½”ì¹˜ ìŠ¤í”½ìŠ¤!')
st.caption('ì…ë ¥ ì˜ˆì‹œ ì…ë‹ˆë‹¤.')
st.caption('ìê¸° ì†Œê°œ :ì €ëŠ” ì»´í“¨í„° ê³µí•™ì„ ì „ê³µí•œ ì‹ ì… ê°œë°œìì…ë‹ˆë‹¤. í•™êµì—ì„œëŠ” Pythonê³¼ Javaë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì˜¤í”ˆ ì†ŒìŠ¤ í”„ë¡œì íŠ¸ì— ì°¸ì—¬í•˜ì—¬ ì‹¤ì œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²½í—˜ì„ í–ˆìŠµë‹ˆë‹¤. íŒ€ì›Œí¬ì™€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥ì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ë©°, ëŠ˜ ìƒˆë¡œìš´ ê²ƒì„ ë°°ìš°ê³  ì„±ì¥í•˜ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤.')
st.caption('ìƒí™© ì„¤ëª… : ì•„ë˜ ì±„ìš© ê³µê³ ë¥¼ ì½ê³  ë©´ì ‘ì„ ê°€ëŠ” ìƒí™©ì…ë‹ˆë‹¤. ìš°ë¦¬ íšŒì‚¬ëŠ” ì—­ë™ì ì¸ ê°œë°œ íŒ€ì„ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ Javaì™€ Pythonì„ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì›¹ ê°œë°œìë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤. í•„ìˆ˜ ìš”ê±´ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:')
st.caption('1. ì»´í“¨í„° ê³µí•™ ë˜ëŠ” ê´€ë ¨ ë¶„ì•¼ì˜ í•™ì‚¬ ì´ìƒì˜ í•™ìœ„ 2. Python, Javaì— ëŒ€í•œ ê¹Šì€ ì´í•´ 3. Gitê³¼ ê°™ì€ ë²„ì „ ê´€ë¦¬ ë„êµ¬ ì‚¬ìš© ê²½í—˜ 4. íŒ€ì›Œí¬ì™€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥ 5. RESTful API ê°œë°œ ê²½í—˜ ìš°ëŒ€ì‚¬í•­: 1. í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤(AWS, Azure ë“±) ì‚¬ìš© ê²½í—˜ 2. CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê²½í—˜ ')

# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì •
if 'show_questions' not in st.session_state:
    st.session_state.show_questions = False
if 'show_answer_input' not in st.session_state:
    st.session_state.show_answer_input = True
if 'recomendq' not in st.session_state:
    st.session_state.recomendq = "ê¸°ë³¸ ì˜ˆìƒ ì§ˆë¬¸" #ì´ˆê¸°ê°’

#ì œëª©
st.write("ìê¸°ì†Œê°œì„œ ì—…ë¡œë“œì‹œ ìƒì„¸í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

#uploader
uploaded_file = st.file_uploader("ìê¸°ì†Œê°œì„œë¥¼ PDF ë˜ëŠ” TXT íŒŒì¼ë¡œ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”",type=['pdf', 'txt'])
st.write("___")

# Before the function definition
if uploaded_file is not None:
    file_extension = os.path.splitext(uploaded_file.name)[1].lower()
else:
    file_extension = ""

def file_to_document(uploaded_file):
    # PDF íŒŒì¼ì˜ ê²½ìš°
    if file_extension == '.pdf':
        temp_dir = tempfile.TemporaryDirectory()
        temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)
        with open(temp_filepath,"wb") as f:
            f.write(uploaded_file.getvalue())
        loader = PyPDFLoader(temp_filepath)
        pages = loader.load_and_split()
        return pages

    # TXT íŒŒì¼ì˜ ê²½ìš°
    elif file_extension == '.txt':
        temp_dir = tempfile.TemporaryDirectory()
        temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)
        with open(temp_filepath,"wb") as f:
            f.write(uploaded_file.getvalue())
        loader = TextLoader(temp_filepath)
        pages = loader.load_and_split()
        return pages

    else:
        raise ValueError("PDF, TXTë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤..")

#ì—…ë¡œë“œì‹œ ë™ì‘ ì½”ë“œ
if st.button('ìê¸°ì†Œê°œì„œ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±'):
    with st.spinner('ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):
        if uploaded_file is not None:
            pages = file_to_document(uploaded_file)

            #Split
            text_splitter = RecursiveCharacterTextSplitter(
            # Set a really small chunk size, just to show.
                chunk_size = 100,
                chunk_overlap  = 20,
                length_function = len,
                is_separator_regex = False,
            )
            texts = text_splitter.split_documents(pages)

            #Embedding

            embeddings_model = OpenAIEmbeddings()
            
        
            #load it into Chroma
            data = Chroma.from_documents(texts,embeddings_model)

            # # summurize texts
            # chain = load_summarize_chain(chat_model, chain_type="stuff")
            # docs = chain.run(texts)

            #Stream ë°›ì•„ ì¤„ Hander ë§Œë“¤ê¸°
            from langchain.callbacks.base import BaseCallbackHandler
            class StreamHandler(BaseCallbackHandler):
                def __init__(self, container, initial_text=""):
                    self.container = container
                    self.text=initial_text
                def on_llm_new_token(self, token: str, **kwargs) -> None:
                    self.text+=token
                    self.container.markdown(self.text)

            #ìê¸°ì†Œê°œì„œ ìš”ì•½

            st.header("ìê¸°ì†Œê°œì„œ ìš”ì•½")

            chat_box = st.empty()
            stream_hander = StreamHandler(chat_box)

            # Define prompt
            prompt_template = """ì•„ë˜ ë‚´ìš©ì— ëŒ€í•œ 2000 ì ì´ë‚´ ìš”ì•½ì„ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì„¸ìš”:
            "{text}"
            ìš”ì•½:"""
            prompt = PromptTemplate.from_template(prompt_template)

            # Define LLM chain
            llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo-16k", streaming=True, callbacks=[stream_hander])
            llm_chain = LLMChain(llm=llm, prompt=prompt)

            # Define StuffDocumentsChain
            stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="text")
            summary=stuff_chain.run(texts)
                    

            personaq ="ìœ„ ìê¸°ì†Œê°œì„œ ìš”ì•½ì„ ì½ê³  ë©´ì ‘ê´€ ì…ì¥ì—ì„œ ì§€ì›ìì— ëŒ€í•œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
            qa_chain = RetrievalQA.from_chain_type(chat_model, retriever=data.as_retriever())
            result = qa_chain({"query" : summary + personaq})
            st.write(result["result"])
            st.write("[AIì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì½”ì¹˜ ìŠ¤í”½ìŠ¤ì™€ í•¨ê»˜ ìŠ¤í”¼ì¹˜  ëª¨ì„ í•˜ê¸°](https://open.kakao.com/o/ghKHP4If)")
        else:
            st.warning("ìê¸°ì†Œê°œì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")




col1, col2 = st.columns(2)
with col1:
    person = st.text_area('ìê¸° ì†Œê°œ', help='ìê¸°ì†Œê°œë¥¼ ì—…ë¡œë“œí•˜ì§€ ì•Šìœ¼ì…¨ë‹¤ë©´ ì—¬ê¸°ì— ê°„ë‹¨í•˜ê²Œ ì ì–´ì£¼ì„¸ìš”.')

with col2:
    description = st.text_area('ìƒí™© ì„¤ëª…', help='ì–´ë–¤ ìƒí™©ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”')


if st.button('ì˜ˆìƒ ì§ˆë¬¸ ìƒì„±'):
    with st.spinner('ì§ˆë¬¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...ì˜ˆìƒ 10ì´ˆ?!'):
        st.session_state.recomendq = chat_model.predict(person +"ì€ ì œì¶œëœ ìê¸°ì†Œê°œì„œì´ë‹¤." + description + "ì¸ ìƒí™©ì„ ê¸°ë°˜ìœ¼ë¡œ 1ë¶„ë™ì•ˆ ë‹µë³€í• ë§Œí•œ ìƒëŒ€ë°©ì˜ ì§ˆë¬¸ 1ê°œì™€ ì˜ˆìƒ ë‹µë³€ì„ ë§Œë“¤ì–´ì¤˜")
        st.session_state.show_questions = True
        st.session_state.show_answer_input = True

# ì˜ˆìƒ ì§ˆë¬¸ í‘œì‹œ
if st.session_state.show_questions:
    st.write('ì˜ˆìƒì§ˆë¬¸:', st.session_state.recomendq)

if st.session_state.show_answer_input:
    question = st.text_area('ì§ˆë¬¸', value=st.session_state.recomendq if st.session_state.show_questions else "")
    st.text('ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”')
    answer = st.text_area('ë‹µë³€ ì…ë ¥')

    if st.button('ë¶„ì„ ì‹œì‘'):
        with st.spinner('ë‹µë³€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...ìµœëŒ€ 1ë¶„?!'):
            chat_box = st.empty()
            stream_hander = StreamHandler(chat_box)
            chat_model = ChatOpenAI(model_name="gpt-4", temperature=0, streaming=True, callbacks=[stream_hander])
            result = chat_model.predict("ì´ìš©ìê°€" +person + "ì™€" + description + "ì„ ë°”íƒ•ìœ¼ë¡œ" + question + "ì— ëŒ€í•´" + answer + "ìœ¼ë¡œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤. ì´ ë‹µë³€ì— ëŒ€í•´ ëª…í™•ì„±, êµ¬ì¡°í™”, ì ì ˆí•œ ê¸¸ì´, ë¬¸ë²•ê³¼ ì–¸ì–´ ì‚¬ìš©, ê°ì •ì˜ í‘œí˜„, ì»¨í…ìŠ¤íŠ¸ ì´í•´, ê·¸ë¦¬ê³  ì‘ìš© ë° ì˜ˆì‹œ ì‚¬ìš©ì˜ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”." +  "ìœ„ ë‹µë³€ì— ëŒ€í•´ì„œ ë‹µë³€ ê°œì„  ì•ˆì„ ë³´ì—¬ì£¼ì„¸ìš”.")
            st.write('ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ ëª¨ë²” ë‹µë³€ì€?', result)
else:
    st.write('ëª¨ë²”ë‹µë³€ì„ ë°›ìœ¼ë ¤ë©´ í´ë¦­')


if st.button("ë¦¬ì…‹"):
    st.session_state.show_questions = False
    st.session_state.show_answer_input = True
    st.session_state.recomendq = ""


import openai
import streamlit as st


st.title("ğŸ’¬ ëª¨ì˜ ë©´ì ‘í•˜ê¸°")
st.caption("ğŸš€ ìŠ¤í”½ìŠ¤ ëª¨ì˜ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.")

# ì‚¬ìš©ìë¡œë¶€í„° personê³¼ descriptionì„ ì…ë ¥ë°›ëŠ” ì½”ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.
person = st.text_input("ìê¸° ì†Œê°œ")
description = st.text_input("ìƒí™© ì„¤ëª…")

if st.button('ì…ë ¥ëœ ë‚´ìš© ê¸°ë°˜ ëª¨ì˜ ë©´ì ‘ ì‹œì‘'):
    interveiwer = person + description + "ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì „ë¬¸ ë©´ì ‘ê´€ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. ë‹µë³€ì€ í•œê¸€ë¡œ í•œë‹¤. ì´ì œ 'ì•ˆë…•í•˜ì„¸ìš”. ë©´ì ‘ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.'ë¼ëŠ” ë§ë¡œ ë©´ì ‘ì„ ë°”ë¡œ ì‹œì‘í•œë‹¤."
    st.session_state["messages"] = [{"role": "user", "content": interveiwer}]
else:
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "user", "content": "ì „ë¬¸ ë©´ì ‘ê´€ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. ë‹µë³€ì€ í•œê¸€ë¡œ í•œë‹¤. ì´ì œ 'ì•ˆë…•í•˜ì„¸ìš”. ë©´ì ‘ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.'ë¼ëŠ” ë§ë¡œ ë©´ì ‘ì„ ë°”ë¡œ ì‹œì‘í•œë‹¤."}]

# ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.get("messages", []):
    st.chat_message(message["role"]).write(message["content"])

# ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.
if user_input := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # ì±—ë´‡ì˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    response = openai.ChatCompletion.create(model="gpt-4", messages=st.session_state.messages)
    msg = response.choices[0].message
    st.session_state.messages.append(msg)
    


# if prompt := st.chat_input():
    
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     st.chat_message("user").write(prompt)

#     st.chat_message("assistant").write(msg.content)